{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']=r'/home/ritika/Downloads/spark-3.0.0-bin-hadoop2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.0-1-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 241 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Requirement already satisfied: six in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: cycler, kiwisolver, pillow, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.0 pillow-7.2.0\n",
      "Requirement already satisfied: pandas in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ritika/anaconda3/envs/pyspark1/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as pyrandom\n",
    "import matplotlib\n",
    "matplotlib.rc(\"image\",cmap=\"gray\")\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"kMeansWithSpark\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a random dataset which we will use to write cluster algo with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[ 45.3, -17.5],\n",
    "                              [ 42.4, -12.5],\n",
    "                              [28., -23.9],\n",
    "                              [29.5, -19.0],\n",
    "                              [32.8, -18.84],\n",
    "                            [23.8, -16.45],\n",
    "                            [22.3, -15.54],\n",
    "                            [12.8, -13.84]\n",
    "                             ],\n",
    "                              [\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|   x|     y|\n",
      "+----+------+\n",
      "|45.3| -17.5|\n",
      "|42.4| -12.5|\n",
      "|28.0| -23.9|\n",
      "|29.5| -19.0|\n",
      "|32.8|-18.84|\n",
      "|23.8|-16.45|\n",
      "|22.3|-15.54|\n",
      "|12.8|-13.84|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------+\n",
      "|   x|     y|     features|\n",
      "+----+------+-------------+\n",
      "|45.3| -17.5| [45.3,-17.5]|\n",
      "|42.4| -12.5| [42.4,-12.5]|\n",
      "|28.0| -23.9| [28.0,-23.9]|\n",
      "|29.5| -19.0| [29.5,-19.0]|\n",
      "|32.8|-18.84|[32.8,-18.84]|\n",
      "|23.8|-16.45|[23.8,-16.45]|\n",
      "|22.3|-15.54|[22.3,-15.54]|\n",
      "|12.8|-13.84|[12.8,-13.84]|\n",
      "+----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------+----------+\n",
      "|   x|     y|     features|prediction|\n",
      "+----+------+-------------+----------+\n",
      "|45.3| -17.5| [45.3,-17.5]|         1|\n",
      "|42.4| -12.5| [42.4,-12.5]|         1|\n",
      "|28.0| -23.9| [28.0,-23.9]|         0|\n",
      "|29.5| -19.0| [29.5,-19.0]|         0|\n",
      "|32.8|-18.84|[32.8,-18.84]|         1|\n",
      "|23.8|-16.45|[23.8,-16.45]|         0|\n",
      "|22.3|-15.54|[22.3,-15.54]|         0|\n",
      "|12.8|-13.84|[12.8,-13.84]|         0|\n",
      "+----+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(new_df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
